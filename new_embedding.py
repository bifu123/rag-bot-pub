import os
import sys
from sys import argv
import shutil
import base64
import re
import time


# æ–‡æ¡£åŠ å·¥
from langchain_community.document_loaders import DirectoryLoader, UnstructuredWordDocumentLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader, PythonLoader # æ–‡æ¡£ç±»åŠ è½½å™¨
from langchain_community.document_loaders import SitemapLoader # ç«™ç‚¹åœ°å›¾åŠ è½½ 
from langchain_community.document_loaders import WebBaseLoader # å•ä¸ªURLåŠ è½½
from langchain_community.document_loaders import UnstructuredURLLoader # å¤šURLåˆ—è¡¨åŠ è½½
from langchain_community.document_loaders import SeleniumURLLoader # å¤šURLåˆ—è¡¨åŠ è½½ï¼ˆå«JSï¼‰

from langchain.indexes.vectorstore import VectorstoreIndexCreator
from langchain.text_splitter import RecursiveCharacterTextSplitter # åˆ†å‰²æ–‡æ¡£
from langchain_community.vectorstores import Chroma # é‡åŒ–æ–‡æ¡£æ•°æ®åº“

# ä»æ–‡ä»¶å¯¼å…¥
from models_load import *
from send import *
from sqlite_helper import *


# å¼‚æ­¥å‡½æ•°
import asyncio

# è§£æç«™ç‚¹åœ°å›¾
import xml.etree.ElementTree as ET
import xml.dom.minidom
import datetime
from urllib import request
from bs4 import BeautifulSoup




####################### å¤„ç†ä¼ å‚ 
# è·å–ä¼ å‚
print(f"æ¥æ”¶åˆ°çš„å‚æ•°ï¼š{sys.argv}")
embedding_data_path = sys.argv[1]
embedding_db_path = sys.argv[2]
source_id = str(sys.argv[3])
chat_type = str(sys.argv[4])
user_id = str(sys.argv[5])
group_id = str(sys.argv[6])
at = str(sys.argv[7])
embedding_type = str(sys.argv[8])
bot_nick_name = str(sys.argv[9])
user_nick_name = str(sys.argv[10])
try:
    site_url = str(sys.argv[11])
    site_url = json.loads(base64.b64decode(site_url).decode())
except:
    site_url = False



# æ‰“å°å‚æ•°
print("*" * 40)
print("embedding_data_path:",embedding_data_path)
print("embedding_db_path:",embedding_db_path)
print("source_id:",source_id)
print("chat_type:",chat_type)
print("user_id:",user_id)
print("group_id:",group_id)
print("at:",at)
print("embedding_type:",embedding_type)
print("bot_nick_name:",bot_nick_name)
print("user_nick_name:",user_nick_name)
print("site_url:",site_url)
print("*" * 40)


####################### é‡åŒ–æ¨¡å‹ 

# æœ¬åœ°é‡åŒ–æ¨¡å‹
embedding_ollama = OllamaEmbeddings(
    base_url = embedding_ollama_conf["base_url"], 
    model = embedding_ollama_conf["model"]
) 


# é€‰æ‹©é‡åŒ–æ¨¡å‹
if model_choice["embedding"] == "ollama":
    embedding = embedding_ollama
# else:
#     embedding = embedding_google




####################### å‡½æ•° 
def build_sitemap(URL, source_id):
    '''æ‰€æœ‰urlåˆ—è¡¨'''
    URL_LIST = {}

    '''æ¨¡æ‹Ÿheader'''
    HEADER = {
        'Cookie': 'AD_RS_COOKIE=20080917',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \ AppleWeb\Kit/537.36 (KHTML, like Gecko)\ '
                      'Chrome/58.0.3029.110 Safari/537.36'}

    def get_http(url, headers=None, charset='gbk'):
        if headers is None:
            headers = {}
        try:
            response = request.urlopen(request.Request(url=url, headers=headers))
            return response.read().decode(charset, errors='ignore')
        except Exception as e:
            print("Error occurred while fetching URL:", e)
        return ''

    def open_url(url):
        """
        æ‰“å¼€é“¾æ¥ï¼Œå¹¶è¿”å›è¯¥é“¾æ¥ä¸‹çš„æ‰€æœ‰é“¾æ¥
        :param url:
        :return:
        """
        soup = BeautifulSoup(get_http(url=url, headers=HEADER), 'html.parser')

        all_a = soup.find_all('a')
        url_list = {}
        for a_i in all_a:
            href = a_i.get('href')
            if href is not None and foreign_chain(href):
                url_list[href] = href
                URL_LIST[href] = href
        return url_list

    def foreign_chain(url):
        """
        éªŒè¯æ˜¯å¦æ˜¯å¤–é“¾
        :param url:
        :return:
        """
        return url.find(URL) == 0

    '''é¦–é¡µ'''
    home_all_url = open_url(URL)

    '''å¾ªç¯é¦–é¡µä¸‹çš„æ‰€æœ‰é“¾æ¥'''
    if isinstance(home_all_url, dict):
        # å¾ªç¯é¦–é¡µä¸‹çš„æ‰€æœ‰é“¾æ¥
        for home_url in home_all_url:
            # éªŒè¯æ˜¯å¦æ˜¯æœ¬ç«™åŸŸå
            if foreign_chain(home_url) is True:
                open_url(home_url)

    URL_LIST_COPY = URL_LIST.copy()

    for copy_i in URL_LIST_COPY:
        open_url(copy_i)

    # åˆ›å»ºæ–‡ä»¶
    doc = xml.dom.minidom.Document()
    root = doc.createElement('urlset')
    # è®¾ç½®æ ¹èŠ‚ç‚¹çš„å±æ€§
    root.setAttribute('xmlns:xsi', 'http://www.w3.org/2001/XMLSchema-instance')
    root.setAttribute('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    root.setAttribute('xsi:schemaLocation', 'http://www.sitemaps.org/schemas/sitemap/0.9 '
                                             'http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd')
    doc.appendChild(root)

    for url_list_i in URL_LIST:
        nodeUrl = doc.createElement('url')
        nodeLoc = doc.createElement('loc')
        nodeLoc.appendChild(doc.createTextNode(str(url_list_i)))
        nodeLastmod = doc.createElement("lastmod")
        nodeLastmod.appendChild(doc.createTextNode(str(datetime.datetime.now().date())))
        nodePriority = doc.createElement("priority")
        nodePriority.appendChild(doc.createTextNode('1.0'))
        nodeUrl.appendChild(nodeLoc)
        nodeUrl.appendChild(nodeLastmod)
        nodeUrl.appendChild(nodePriority)
        root.appendChild(nodeUrl)

    sitemap_file = f'{source_id}.xml'

    with open(f'{sitemap_file}', 'w', encoding="utf-8") as fp:
        doc.writexml(fp, indent='\t', addindent='\t', newl='\n')

    return {"url":URL, "sitemap": f"{sitemap_file}"}

# è§£æç«™ç‚¹åœ°å›¾çš„å‡½æ•°
def parse_sitemap(xml_file):
    urls = []
    # è§£æ XML æ–‡ä»¶
    tree = ET.parse(xml_file)
    root = tree.getroot()
    # å®šä¹‰ XML å‘½åç©ºé—´
    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
    # æŸ¥æ‰¾æ‰€æœ‰ URL æ¡ç›®
    for url in root.findall('ns:url', namespace):
        print(f"æ­£åœ¨è§£æï¼š{url}")
        loc = url.find('ns:loc', namespace).text
        lastmod = url.find('ns:lastmod', namespace).text
        priority = url.find('ns:priority', namespace).text
        #urls.append({'loc': loc, 'lastmod': lastmod, 'priority': priority})
        urls.append(loc)
    print(f"è§£æå®Œæ¯•ï¼š{url}")
    return urls
# åŠ è½½ç«™ç‚¹åœ°å›¾çš„å‡½æ•°
def get_loaders_from_sitemap(urls):
    documents = SeleniumURLLoader(urls)
    loaders = documents.load()
    return loaders
# åŠ è½½æ–‡æ¡£çš„å‡½æ•°
def get_loads_from_dir(new_embedding_db_path):
    print("æ­£åœ¨åŠ è½½" + new_embedding_db_path + "ä¸‹çš„æ‰€æœ‰æ–‡æ¡£...")
    loader = DirectoryLoader(new_embedding_db_path, show_progress=True, use_multithreading=True)
    loaders = loader.load()
    print(loaders)
    return loaders



####################### æ‰§è¡Œè¿‡ç¨‹
# ç¡®å®šé‡åŒ–å­˜å‚¨è·¯å¾„
# åˆ é™¤æ—§å‘é‡å­˜å‚¨æ–‡ä»¶å¤¹
if os.path.exists(embedding_db_path) and os.path.isdir(embedding_db_path):
    try:
        shutil.rmtree(embedding_db_path)
        print(f"æ–‡ä»¶å¤¹ '{embedding_db_path}' å·²æˆåŠŸåˆ é™¤ï¼Œå³å°†æ›´æ–°æ•°æ®...\n")
        new_embedding_db_path = embedding_db_path
    except OSError as e:
        # åˆ¤æ–­å½“å‰è·¯å¾„ä¸­æ˜¯å¦åŒ…å«"_1"
        if "_1" in f"{embedding_db_path}":
            new_embedding_db_path = embedding_db_path.replace("_1", "")
        else:
            new_embedding_db_path = embedding_db_path + "_1"
            print(f"åˆ é™¤æ–‡ä»¶å¤¹ '{embedding_db_path}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}ï¼Œå°†æ›´æ”¹è·¯å¾„ä¸º{new_embedding_db_path}\n")
else:
    new_embedding_db_path = embedding_db_path
print(f"{embedding_db_path} ä¸å­˜åœ¨ï¼Œä¸éœ€åˆ é™¤ã€‚å°†ä½¿ç”¨ {new_embedding_db_path} ä½œä¸ºå‘é‡å­˜å‚¨è·¯å¾„")


# å°†æ–°è·¯å¾„æ›´æ–°åˆ°æ•°æ®åº“
if site_url:
    update_db_path_site(source_id, new_embedding_db_path)
else:
    update_db_path(source_id, new_embedding_db_path)




# æ ¹æ®ä¼ å‚å†³å®šåŠ è½½å™¨æ¥åŠ è½½æ–‡æ¡£
if embedding_type == "site":
    # åˆ¶ä½œç«™ç‚¹åœ°å›¾
    print(f"æ­£åœ¨åˆ¶ä½œç«™ç‚¹åœ°å›¾...{site_url}")
    sitemap_file = build_sitemap(site_url, source_id)["sitemap"]
    # è§£æç«™ç‚¹åœ°å›¾
    print(f"æ­£åœ¨è§£æç«™ç‚¹åœ°å›¾...{sitemap_file}")
    urls = parse_sitemap(sitemap_file)
    # åŠ è½½ç½‘ç«™URLåˆ—è¡¨
    print(f"åŠ è½½ç½‘ç«™URLåˆ—è¡¨...")
    loaders = get_loaders_from_sitemap(urls)
elif embedding_type == "file":
    try:
        loaders = get_loads_from_dir(embedding_data_path)
    except Exception as e:
        print(f"get_loads_from_dirå‡ºé”™ï¼š{e}")
        #time.sleep(1000000)
print(loaders)
#time.sleep(1000000)


# åˆ†å‰²æ–‡æ¡£
print("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_overlap, chunk_overlap=chunk_overlap)
all_splits = text_splitter.split_documents(loaders)

# ä¿å­˜å‘é‡
print("æ­£åœ¨ä¿å­˜å‘é‡...")
Chroma.from_documents(
    documents =all_splits,
    embedding = embedding,
    persist_directory = new_embedding_db_path
)

# æ„å»ºæ¶ˆæ¯å†…å®¹
response_message = f"é‡åŒ–æ‰§è¡Œç»“æŸï¼Œå·²è¿ç§»è‡³æ–°çŸ¥è¯†åº“ï¼š{new_embedding_db_path}ğŸ˜Š"

# å‘é€æ¶ˆæ¯
asyncio.run(answer_action(chat_type, user_id, group_id, at, response_message))


# time.sleep(1000000)
# input("Press Enter to exit...")

