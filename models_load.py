import os
import sys
import requests
import json

from config import *
from sqlite_helper import *


# ollamaæ¨¡å‹
from langchain_community.embeddings import OllamaEmbeddings # é‡åŒ–æ–‡æ¡£
from langchain_community.llms import Ollama #æ¨¡å‹

# æç¤ºè¯æ¨¡æ¿
from langchain_core.prompts import ChatPromptTemplate

# geminiæ¨¡å‹
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

# é€šä¹‰åƒé—®æ¨¡å‹
from langchain_community.llms import Tongyi
import dashscope

# è¯­ä¹‰æ£€ç´¢
from langchain.schema.runnable import RunnableMap
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# chatGLM3-6B æ¨¡å‹
from langchain_community.llms.chatglm3 import ChatGLM3

# kimi æ¨¡å‹
from langchain_community.llms.moonshot import Moonshot

# groq api æ¨¡å‹
from langchain_groq import ChatGroq

# å¼‚æ­¥å‡½æ•°
import asyncio




############################# API KEY #################################
# å°†å„ä¸ªåœ¨çº¿æ¨¡å‹ API key åŠ å…¥ç¯å¢ƒå˜é‡
os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY
os.environ['DASHSCOPE_API_KEY'] = DASHSCOPE_API_KEY
os.environ["MOONSHOT_API_KEY"] = MOONSHOT_API_KEY
os.environ["GROQ_API_KEY"] = GROQ_API_KEY
############################# é‡åŒ–æ¨¡å‹ #################################
# æœ¬åœ°é‡åŒ–æ¨¡å‹
embedding_ollama = OllamaEmbeddings(
    base_url = embedding_ollama_conf["base_url"], 
    model = embedding_ollama_conf["model"]
) 
# çº¿ä¸Šgoogleé‡åŒ–æ¨¡å‹
embedding_google = GoogleGenerativeAIEmbeddings(
    model = embedding_google_conf["model"]
) 
# embedding_google.embed_query("hello, world!")
############################# è¯­è¨€æ¨¡å‹ #################################
# æœ¬åœ°è¯­è¨€æ¨¡å‹
llm_ollama = Ollama(
    base_url = llm_ollama_conf["base_url"], 
    model = llm_ollama_conf["model"]
)
# åœ¨çº¿è¯­è¨€æ¨¡å‹ gemini
llm_gemini = ChatGoogleGenerativeAI(
    model = llm_gemini_conf["model"],
    temperature = llm_gemini_conf["temperature"]
) 
# åœ¨çº¿è¯­è¨€æ¨¡å‹ é€šä¹‰åƒé—®
llm_tongyi = Tongyi(
    model_name = llm_tongyi_conf["model_name"],
    temperature = llm_tongyi_conf["temperature"],
    streaming = llm_tongyi_conf["streaming"]
    #enable_search = True
) 
# åœ¨çº¿è¯­è¨€æ¨¡å‹ kimi
llm_kimi = Moonshot(
    model_name = llm_kimi_conf["model_name"],
    temperature = llm_kimi_conf["temperature"]
) 
# åœ¨çº¿è¯­è¨€æ¨¡å‹ groq
llm_groq = ChatGroq(
    model_name = llm_groq_conf["model_name"],
    temperature = llm_groq_conf["temperature"]
) 
# æœ¬åœ°è¯­è¨€æ¨¡å‹ ChatGLM3
llm_chatGLM = ChatGLM3(
    endpoint_url = llm_chatGLM_conf["endpoint_url"],
    max_tokens = llm_chatGLM_conf["max_tokens"],
    top_p = llm_chatGLM_conf["top_p"]
)

############################# æ¨¡å‹é€‰æ‹© #################################
# è¯»å–æ•°æ®åº“
def get_models_on_request():
    models = get_models_table()
    must_use_llm_rag = models["must_use_llm_rag"]
    # é€‰æ‹©é‡åŒ–æ¨¡å‹
    if models["embedding"] == "ollama":
        embedding = embedding_ollama
    else:
        embedding = embedding_google

    # é€‰æ‹©èŠå¤©è¯­è¨€æ¨¡å‹
    if models["llm"] == "ollama":
        llm = llm_ollama
    elif models["llm"] == "gemini": 
        llm = llm_gemini
    elif models["llm"] == "tongyi": 
        llm = llm_tongyi
    elif models["llm"] == "kimi": 
        llm = llm_kimi
    elif models["llm"] == "groq": 
        llm = llm_groq
    else:
        llm = llm_chatGLM

    # é€‰æ‹©çŸ¥è¯†åº“è¯­è¨€æ¨¡å‹
    if models["llm_rag"] == "ollama":
        llm_rag = llm_ollama
    elif models["llm_rag"] == "gemini": 
        llm_rag = llm_gemini
    elif models["llm_rag"] == "tongyi": 
        llm_rag = llm_tongyi
    elif models["llm_rag"] == "kimi": 
        llm_rag = llm_kimi
    elif models["llm_rag"] == "groq": 
        llm_rag = llm_groq
    else:
        llm_rag = llm_chatGLM
        
    return embedding, llm, llm_rag, must_use_llm_rag



############################# æ¨¡å‹æ–¹æ³• #################################
# gemini èŠå¤©
async def chat_gemini(wxid, content, GMI_SERVER_URL):
    print("*" * 40)
    print("æ­£åœ¨å‘llmæäº¤...")
    try:
        response_text = requests.get(GMI_SERVER_URL).text
        json_response = json.loads(response_text)
        reply = json_response.get('reply')
        print("=" * 50, "\n",type(reply), reply)
        response_message = reply
    except Exception as e:
        response_message = f"LLMå“åº”é”™è¯¯: {e}"
    return response_message

# æ ¼å¼åŒ–èŠå¤©è®°å½•
def format_history(bot_nick_name, history):
    system_prompt = {"user": "system", "content": f"ä½ å¥½ï¼Œæˆ‘çš„åå­—å«{bot_nick_name}ï¼Œæˆ‘ä¼šå°½åŠ›è§£ç­”å¤§å®¶çš„é—®é¢˜."}
    result = []
    result.append(system_prompt)
    for item in history:
        result.append({"user": item[0], "content": item[1].replace(at_string, "")})
    return result

# å¤„ç†èŠå¤©è®°å½•
async def do_chat_history(chat_history, source_id, user, content, user_state, name_space):
    history_size_now = sys.getsizeof(f"{chat_history}")
    # å¦‚æœè¶…è¿‡é¢„å®šå­—èŠ‚å¤§å°å°±æ”¾å¼ƒå†™å…¥
    if not history_size_now > chat_history_size_set:
        # æ’å…¥å½“å‰æ•°æ®è¡¨ source_idã€queryã€result
        insert_chat_history(source_id, user, content, user_state, name_space)
        # å°†èŠå¤©è®°å½•å…¥æ—§å½’æ¡£è®°å½•è¡¨history_old.xlsxè¡¨ä¸­
        insert_chat_history_xlsx(source_id, user, content, user_state, name_space)
    else:
        print("è®°å½•è¿‡å¤§ï¼Œæ”¾å¼ƒå†™å…¥")

# å‘é‡æ£€ç´¢èŠå¤©ï¼ˆæ‰§è¡Œå‘é‡é“¾ï¼‰
async def run_chain(bot_nick_name, user_nick_name, retriever, source_id, query, user_state="èŠå¤©", name_space="test"):
    embedding, llm, llm_rag, must_use_llm_rag = get_models_on_request()
    if query !="" and query is not None:
        print("=" * 50)
        print("å½“å‰ä½¿ç”¨çš„çŸ¥è¯†åº“LLMï¼š", llm_rag)
        template_cn = """è¯·æ ¹æ®ä¸Šä¸‹æ–‡å’Œå¯¹è¯å†å²è®°å½•ç”¨ç®€ä½“ä¸­æ–‡å®Œæ•´åœ°å›ç­”é—®é¢˜ Please answer in Simplified Chinese:
        {context}
        {question}
        """
        

        # å¤„ç†èŠå¤©è®°å½•
        data = fetch_chat_history(source_id, user_state, name_space) # ä»æ•°æ®åº“ä¸­æå–source_idçš„èŠå¤©è®°å½•
        chat_history = format_history(bot_nick_name, data)
        
        history_size_now = sys.getsizeof(f"{chat_history}") + sys.getsizeof(f"{query}") # å¦‚æœè¶…è¿‡é¢„å®šå­—èŠ‚å¤§å°ï¼Œåˆ é™¤è®°å½•
        print("=" * 50)
        print(f"é¢„è®¡èŠå¤©è®°å½•å¤§å°ï¼š{history_size_now}\nèŠå¤©è®°å½•ï¼š\n{chat_history}")
        
        while history_size_now > chat_history_size_set:
            if history_size_now > chat_history_size_set:
                delete_oldest_records(source_id, user_state, name_space) # åˆ é™¤æ•°æ®åº“ä¸­æ—¶é—´æœ€æ—§çš„1æ¡è®°å½•
                if chat_history and len(chat_history) > 1:
                    data.pop(0) # åˆ é™¤chat_historyä¸­æ—¶é—´æœ€æ—§çš„1æ¡è®°å½•
                    chat_history = format_history(bot_nick_name, data)
                    history_size_now = sys.getsizeof(f"{chat_history}") + sys.getsizeof(f"{query}")
                    print("å†å²è®°å½•åŠé—®é¢˜å­—èŠ‚ä¹‹å’Œè¶…è¿‡é¢„å®šå€¼ï¼Œåˆ é™¤æ—¶é—´æœ€æ—§çš„1æ¡è®°å½•")
                else:
                    print("èŠå¤©è®°å½•ä¸ºç©ºï¼Œæ— éœ€åˆ é™¤")
                    break
            else:
                break  # å¦‚æœæ¡ä»¶ä¸å†æ»¡è¶³ï¼Œåˆ™è·³å‡ºå¾ªç¯
            
            
        # ç”±æ¨¡æ¿ç”Ÿæˆprompt
        prompt = ChatPromptTemplate.from_template(template_cn) 
        
        # åˆ›å»ºchain
        chain = RunnableMap({
            "context": lambda x: retriever.get_relevant_documents(x["question"]),
            "question": RunnablePassthrough(),
            "chat_history": lambda x: chat_history  # ä½¿ç”¨å†å²è®°å½•çš„æ­¥éª¤
        }) | prompt | llm_rag | StrOutputParser()
        
        # æ‰§è¡Œé—®ç­”
        request = f'{{"user":"{user_nick_name}", "content":"{query}"}}'
        try:
            response_message = chain.invoke(request)
            # å¤„ç†èŠå¤©è®°å½• 
            await do_chat_history(chat_history, source_id, user_nick_name, query, user_state, name_space)
            await do_chat_history(chat_history, source_id, bot_nick_name, response_message, user_state, name_space)
        except Exception as e:
            response_message = f"LLMå“åº”é”™è¯¯: {e}"
            print(f"LLMå“åº”é”™è¯¯: {e}")
            
        # è¿”å›ç»“æœ
        return response_message + "ğŸ˜Š"

# é€šç”¨èŠå¤©
async def chat_generic_langchain(bot_nick_name, user_nick_name, source_id, query, user_state="èŠå¤©",name_space="test"):
    embedding, llm, llm_rag, must_use_llm_rag = get_models_on_request()
    if query !="" and query is not None:
        # å¤„ç†èŠå¤©è®°å½•
        data = fetch_chat_history(source_id, user_state, name_space) # ä»æ•°æ®åº“ä¸­æå–source_idçš„èŠå¤©è®°å½•
        chat_history = format_history(bot_nick_name, data)
        
        history_size_now = sys.getsizeof(f"{chat_history}") + sys.getsizeof(f"{query}") # å¦‚æœè¶…è¿‡é¢„å®šå­—èŠ‚å¤§å°ï¼Œåˆ é™¤è®°å½•
        print("=" * 50)
        print(f"é¢„è®¡èŠå¤©è®°å½•å¤§å°ï¼š{history_size_now}\nèŠå¤©è®°å½•ï¼š\n{chat_history}")
        
        while history_size_now > chat_history_size_set:
            if history_size_now > chat_history_size_set:
                delete_oldest_records(source_id, user_state, name_space) # åˆ é™¤æ•°æ®åº“ä¸­æ—¶é—´æœ€æ—§çš„1æ¡è®°å½•
                if chat_history and len(chat_history) > 1:
                    data.pop(0) # åˆ é™¤chat_historyä¸­æ—¶é—´æœ€æ—§çš„1æ¡è®°å½•
                    chat_history = format_history(bot_nick_name, data)
                    history_size_now = sys.getsizeof(f"{chat_history}") + sys.getsizeof(f"{query}")
                    print("å†å²è®°å½•åŠé—®é¢˜å­—èŠ‚ä¹‹å’Œè¶…è¿‡é¢„å®šå€¼ï¼Œåˆ é™¤æ—¶é—´æœ€æ—§çš„1æ¡è®°å½•")
                else:
                    print("èŠå¤©è®°å½•ä¸ºç©ºï¼Œæ— éœ€åˆ é™¤")
                    break
            else:
                break  # å¦‚æœæ¡ä»¶ä¸å†æ»¡è¶³ï¼Œåˆ™è·³å‡ºå¾ªç¯

    
        # ç”±æ¨¡æ¿ç”Ÿæˆ prompt
        prompt = ChatPromptTemplate.from_template("""
            ä½ æ˜¯ä¸€ä¸ªçƒ­å¿ƒçš„äººï¼Œå°½åŠ›ä¸ºäººä»¬è§£ç­”é—®é¢˜ï¼Œè¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ã€‚Please answer in Simplified Chinese:
            {chat_history}
            {question}
        """)
        print("=" * 50)
        
        # åˆ›å»ºé“¾ï¼Œå°†å†å²è®°å½•ä¼ é€’ç»™é“¾
        if user_state != "èŠå¤©" and must_use_llm_rag == 1:
            chain = {
                "question": RunnablePassthrough(), 
                "chat_history": lambda x: chat_history,
            } | prompt | llm_rag | StrOutputParser()  
            print("å½“å‰ä½¿ç”¨çš„èŠå¤©LLMï¼š", llm_rag)
        else:
            chain = {
                "question": RunnablePassthrough(), 
                "chat_history": lambda x: chat_history,
            } | prompt | llm | StrOutputParser()  
            print("å½“å‰ä½¿ç”¨çš„èŠå¤©LLMï¼š", llm)

        # æ‰§è¡Œé—®ç­”
        request = f'{{"user":"{user_nick_name}", "content":"{query}"}}'
        try:
            response_message = chain.invoke(request)
            # å¤„ç†èŠå¤©è®°å½• 
            await do_chat_history(chat_history, source_id, user_nick_name, query, user_state, name_space)
            await do_chat_history(chat_history, source_id, bot_nick_name, response_message, user_state, name_space)
        except Exception as e:
            response_message = f"LLMå“åº”é”™è¯¯: {e}"
            print(f"LLMå“åº”é”™è¯¯: {e}")
            
        return response_message + "ğŸ˜Š"



